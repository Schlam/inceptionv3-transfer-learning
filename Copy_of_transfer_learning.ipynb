{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy of transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FVI51A5R3kjB"
      },
      "source": [
        "# Image recognition with transfer learning\n",
        "\n",
        "\n",
        "Computer vision has improved rapidly over the past decade due to a few intersecting trends:\n",
        "\n",
        " - Research advancements in machine learning\n",
        " - Petabytes of image data available online\n",
        " - Open Source culture with in AI community\n",
        "\n",
        "\n",
        "One tool in a data scientists toolbox is **transfer learning**. This is particularly useful for image recognition, which typically requires a large amount of training data and extensive computation.\n",
        "\n",
        "\n",
        "Instead of training a model from scratch, we can import the weights from another model a basis for our specific use case.\n",
        "\n",
        "\n",
        "\n",
        "## InceptionV3\n",
        "\n",
        "![](https://miro.medium.com/max/2000/0*te3xksOrVytM-B17)\n",
        "\n",
        "InceptionV3 is one of the most advanced models for computer vision currently available. Created by Google Research members Szegedy et. al. The graphic above shows the complex architechture of this *very* deep network.\n",
        "\n",
        "A link to the paper can be found here: https://arxiv.org/pdf/1512.00567.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssEdw3gIGXx4"
      },
      "source": [
        "\n",
        "..\n",
        "\n",
        "\n",
        "### Transfer Learning with InceptionV3\n",
        "\n",
        "We can take advantage of this model, which has already been trained on thousands of images, and modify it for a specific use case.\n",
        "\n",
        "My mother is an avid birdwatcher, and maybe she'd like to see a bird out in the wild, snap a photo, and see what kind of bird it is.\n",
        "\n",
        "Using the Caltech Birds dataset from 2011, we can train an inception-based model to predict the species of bird pictured in a photograph\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfgEkCOtpvZ6",
        "colab_type": "text"
      },
      "source": [
        "#### Read in data from tensorflow datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwBCMuyies6l",
        "outputId": "c5b26e7d-6f8d-4bb3-ff04-28304de68eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# load Caltech Birds dataset from 2011\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'caltech_birds2011',\n",
        "    split = ['train', 'test'],\n",
        "    shuffle_files = True,\n",
        "    as_supervised = True,\n",
        "    with_info = True)\n",
        "\n",
        "print(\"Dataset info:\\n\\n\", ds_info.description)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset info:\n",
            "\n",
            " Caltech-UCSD Birds 200 (CUB-200) is an image dataset with photos \n",
            "of 200 bird species (mostly North American). The total number of \n",
            "categories of birds is 200 and there are 6033 images in the 2010 \n",
            "dataset and 11,788 images in the 2011 dataset.\n",
            "Annotations include bounding boxes, segmentation labels.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynQ7yy0Vp9qk",
        "colab_type": "text"
      },
      "source": [
        "#### Data preprocessing\n",
        "- normalize each image (256-bit color to float $\\in$ \\[0,1\\] )\n",
        "- pad and rescale (shrink/enlarge dimensiona to 400px, adding white space when nesessary)\n",
        "- batch images (number of images to be held in memory for each epoch )\n",
        "- prefetch (during training step s, allow the data for training step s+1 to be loaded to optimize runtime )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yAuvFxZvsqj8",
        "colab": {}
      },
      "source": [
        "# Set batch size and image dimensions allowed by your memory resources\n",
        "batch_size = 64\n",
        "image_height = 400\n",
        "image_width = 400\n",
        "\n",
        "\n",
        "def norm(image, label):\n",
        "    \"\"\"Returns normalized image\"\"\"\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "def pad(image, label):\n",
        "    \"\"\"Returns resized image, padded where necessary\"\"\"\n",
        "    return tf.image.resize_with_pad(image, image_height, image_width), label\n",
        "\n",
        "\n",
        "# Prepare training data \n",
        "ds_train = ds_train.map(norm, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.map(pad, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.batch(batch_size)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "# Prepare validation data\n",
        "ds_test = ds_test.map(norm, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.map(pad, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(batch_size)\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezUwIZAlqGky",
        "colab_type": "text"
      },
      "source": [
        "#### Load in InceptionV3 model for transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zlq6-QWfsu-m",
        "outputId": "e09e6dd0-dffe-4219-b63e-a6bf72e14374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "pretrained_model = InceptionV3(input_shape=(image_height,image_width,3),\n",
        "                               include_top=False,\n",
        "                               weights='imagenet')\n",
        "\n",
        "# Iterate through layers and make untrainable\n",
        "for layer in pretrained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Flatten the output, and add fully connected layer with a node for each class\n",
        "x = tf.keras.layers.Flatten()(pretrained_model.output)\n",
        "x = tf.keras.layers.Dense(200, activation='softmax')(x)\n",
        "\n",
        "# Use our model as the input layer\n",
        "model = tf.keras.Model(pretrained_model.input, x)\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\"\"\"Uncomment the last line to see the model summary, warning: it is very long\"\"\" \n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Uncomment the last line to see the model summary, warning: it is very long'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nNq-MTa24MHR"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "We being by importing the pretrained model and setting the input shape to match the images we seek to classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUhtLgx6hEUT",
        "colab_type": "code",
        "outputId": "bc52a09c-7969-497a-d9ad-7a508b82d873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=50,\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "94/94 [==============================] - 150s 2s/step - loss: 3.9861 - accuracy: 0.8105\n",
            "Epoch 2/50\n",
            "94/94 [==============================] - 150s 2s/step - loss: 2.0597 - accuracy: 0.8857\n",
            "Epoch 3/50\n",
            "94/94 [==============================] - 147s 2s/step - loss: 1.4462 - accuracy: 0.9117\n",
            "Epoch 4/50\n",
            "94/94 [==============================] - 147s 2s/step - loss: 1.2344 - accuracy: 0.9289\n",
            "Epoch 5/50\n",
            "94/94 [==============================] - 155s 2s/step - loss: 1.2834 - accuracy: 0.9331\n",
            "Epoch 6/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 1.2593 - accuracy: 0.9369\n",
            "Epoch 7/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 1.4441 - accuracy: 0.9298\n",
            "Epoch 8/50\n",
            "94/94 [==============================] - 155s 2s/step - loss: 1.3298 - accuracy: 0.9436\n",
            "Epoch 9/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 1.3134 - accuracy: 0.9441\n",
            "Epoch 10/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 1.1154 - accuracy: 0.9535\n",
            "Epoch 11/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 1.0645 - accuracy: 0.9581\n",
            "Epoch 12/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 1.0570 - accuracy: 0.9606\n",
            "Epoch 13/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.7236 - accuracy: 0.9693\n",
            "Epoch 14/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.7543 - accuracy: 0.9695\n",
            "Epoch 15/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.7682 - accuracy: 0.9683\n",
            "Epoch 16/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.9141 - accuracy: 0.9675\n",
            "Epoch 17/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 0.7052 - accuracy: 0.9731\n",
            "Epoch 18/50\n",
            "94/94 [==============================] - 156s 2s/step - loss: 0.7832 - accuracy: 0.9706\n",
            "Epoch 19/50\n",
            "94/94 [==============================] - 154s 2s/step - loss: 0.8191 - accuracy: 0.9713\n",
            "Epoch 20/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 1.1211 - accuracy: 0.9691\n",
            "Epoch 21/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 0.7200 - accuracy: 0.9770\n",
            "Epoch 22/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.7208 - accuracy: 0.9785\n",
            "Epoch 23/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.6895 - accuracy: 0.9763\n",
            "Epoch 24/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.9248 - accuracy: 0.9736\n",
            "Epoch 25/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 1.1688 - accuracy: 0.9685\n",
            "Epoch 26/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.6580 - accuracy: 0.9798\n",
            "Epoch 27/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 0.5786 - accuracy: 0.9808\n",
            "Epoch 28/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.5421 - accuracy: 0.9833\n",
            "Epoch 29/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.4243 - accuracy: 0.9875\n",
            "Epoch 30/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.2351 - accuracy: 0.9915\n",
            "Epoch 31/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.6115 - accuracy: 0.9838\n",
            "Epoch 32/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.6470 - accuracy: 0.9842\n",
            "Epoch 33/50\n",
            "94/94 [==============================] - 155s 2s/step - loss: 0.3829 - accuracy: 0.9875\n",
            "Epoch 34/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.6178 - accuracy: 0.9825\n",
            "Epoch 35/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 0.8652 - accuracy: 0.9795\n",
            "Epoch 36/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.7265 - accuracy: 0.9800\n",
            "Epoch 37/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.9892 - accuracy: 0.9785\n",
            "Epoch 38/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.8060 - accuracy: 0.9803\n",
            "Epoch 39/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 1.2071 - accuracy: 0.9736\n",
            "Epoch 40/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 0.7063 - accuracy: 0.9821\n",
            "Epoch 41/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.5673 - accuracy: 0.9870\n",
            "Epoch 42/50\n",
            "94/94 [==============================] - 153s 2s/step - loss: 0.9819 - accuracy: 0.9806\n",
            "Epoch 43/50\n",
            "94/94 [==============================] - 154s 2s/step - loss: 0.6911 - accuracy: 0.9830\n",
            "Epoch 44/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.6571 - accuracy: 0.9850\n",
            "Epoch 45/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 1.0630 - accuracy: 0.9786\n",
            "Epoch 46/50\n",
            "94/94 [==============================] - 151s 2s/step - loss: 0.6176 - accuracy: 0.9862\n",
            "Epoch 47/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.4702 - accuracy: 0.9887\n",
            "Epoch 48/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.5360 - accuracy: 0.9872\n",
            "Epoch 49/50\n",
            "94/94 [==============================] - 154s 2s/step - loss: 0.3095 - accuracy: 0.9923\n",
            "Epoch 50/50\n",
            "94/94 [==============================] - 152s 2s/step - loss: 0.4981 - accuracy: 0.9880\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}